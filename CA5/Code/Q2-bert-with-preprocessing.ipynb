{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q2-bert-with-preprocessing.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNWbMbpXXKowS4abv6dHsrm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8JL4OAlSip-W","colab_type":"text"},"source":["# Connect to drive & import libraries"]},{"cell_type":"code","metadata":{"id":"caEGNDMzfRAh","colab_type":"code","outputId":"9feaf375-d42e-4bbc-a8ed-99a1a3594c84","executionInfo":{"status":"ok","timestamp":1588625762743,"user_tz":-270,"elapsed":70660,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tEzV-luAJRsZ","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuOEBW9Liwfv","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import os\n","from keras.engine import Layer\n","import matplotlib.pyplot as plt\n","import keras\n","import numpy as np\n","import nltk\n","import tqdm\n","import tensorflow_hub as hub\n","import string\n","import re\n","from keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGuIwsXXJekG","colab_type":"code","outputId":"a862a04a-b12f-44ce-c801-1f15a8aaccc6","executionInfo":{"status":"ok","timestamp":1588625849999,"user_tz":-270,"elapsed":1626,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tf.__version__"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0-rc3'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"ps76V6J_edIs","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn import metrics"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNTXCjhyQFp7","colab_type":"code","outputId":"9e455cc8-0c62-4e24-c138-bd2aa8616db8","executionInfo":{"status":"ok","timestamp":1588625862804,"user_tz":-270,"elapsed":9564,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1sQU4wuWtkaf","colab_type":"code","outputId":"c5d2d46f-1c9b-4eeb-8c2d-d07fd30ec6ef","executionInfo":{"status":"ok","timestamp":1588625986336,"user_tz":-270,"elapsed":2342,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"pqHDaPXXop-k","colab_type":"code","outputId":"f777090e-63a5-4abb-9099-e6dfdf157ce6","executionInfo":{"status":"ok","timestamp":1588625995197,"user_tz":-270,"elapsed":10602,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":540}},"source":["!pip install bert-for-tf2"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting bert-for-tf2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n","\r\u001b[K     |████████                        | 10kB 30.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.8MB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n","Collecting params-flow>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.38.0)\n","Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=cfedc2c7396dbb05d20a283c5471ebc17249278e2ea4136f6ff33cb470298ccc\n","  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=6f334bcef6ac829ce39c8b72a669b756346b77d49b6c47ee9c98fe0b8ce8c6d9\n","  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=ac02915d822f790bc99e9d71f57d69d7ebcdcf12126eee59aa6e1b1f1d91d602\n","  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n","Successfully built bert-for-tf2 py-params params-flow\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vybgl8VvoqOG","colab_type":"code","colab":{}},"source":["import bert"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b2ADngPjIZo","colab_type":"text"},"source":["# Constants"]},{"cell_type":"code","metadata":{"id":"zxO3Fs12ivVm","colab_type":"code","colab":{}},"source":["# data parameters\n","dataset_address = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","test_data_percentage = 0.2\n","# model parameters\n","learning_rate = 0.0002\n","batch_size = 32\n","max_sequence_length = 128\n","number_of_epochs = [1, 10, 20, 50]\n","# model links\n","bert_model_hub_link = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n","elmo_model_hub_link = 'https://tfhub.dev/google/elmo/3'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NwVmAJGi0vh","colab_type":"text"},"source":["# Reading & Pre-processing the data"]},{"cell_type":"code","metadata":{"id":"fI056jLjj0oU","colab_type":"code","colab":{}},"source":["def load_directory_data(directory):\n","    data = {}\n","    data[\"sentence\"] = []\n","    data[\"sentiment\"] = []\n","    for file_path in os.listdir(directory):\n","        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","            data[\"sentence\"].append(f.read())\n","            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","    return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","    pos_df[\"polarity\"] = 1\n","    neg_df[\"polarity\"] = 0\n","    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","    dataset = tf.keras.utils.get_file(\n","        fname=\"aclImdb.tar.gz\", \n","        origin=dataset_address, \n","        extract=True)\n","  \n","    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                         \"aclImdb\", \"train\"))\n","    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                          \"aclImdb\", \"test\"))\n","    return train_df, test_df\n"," \n","train, test = download_and_load_datasets()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SjOfHPSgtuk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"773df734-d388-4d65-ab44-46d3c89f69a9","executionInfo":{"status":"ok","timestamp":1588626654564,"user_tz":-270,"elapsed":1482,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}}},"source":["train_X = train.sentence.values\n","train_Y = train.polarity.values\n","\n","test_X = test.sentence.values\n","test_Y = test.polarity.values\n","print(len(train_X), len(test_X))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["25000 25000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQ_r5tFLnH7O","colab_type":"text"},"source":["## Create bert tokenizer"]},{"cell_type":"code","metadata":{"id":"VO_FMTtnnMCY","colab_type":"code","colab":{}},"source":["bert_layer = hub.KerasLayer(bert_model_hub_link)\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","bert_tokenizer_instance = bert.bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYuXzRGY1DpG","colab_type":"text"},"source":["### Bert functions for mask and id and segment id"]},{"cell_type":"code","metadata":{"id":"W-UjqYR71I5I","colab_type":"code","colab":{}},"source":["def convert_text_to_number_and_fix_length(this_x, tokenizer_mod):\n","  this_x_ids = tokenizer_mod.convert_tokens_to_ids(this_x,)\n","  if len(this_x_ids) >= max_sequence_length:\n","    this_x_fix_len = this_x_ids[:max_sequence_length]\n","  else:\n","    this_x_fix_len = this_x_ids + [0]*(max_sequence_length - len(this_x_ids)) \n","  return this_x_fix_len\n","\n","def create_mask_for_input(this_x):\n","  if len(this_x) >= max_sequence_length:\n","    this_x_masks = [1]*max_sequence_length\n","  else:\n","    this_x_masks = [1]*len(this_x) + [0] * (max_sequence_length - len(this_x))\n","  return this_x_masks\n","\n","def create_req_bert_inputs(all_X, tokenizer_mod):\n","  X_input_ids, X_input_masks, X_segment_ids = [], [], []\n","\n","  for this_x in tqdm.tqdm(all_X, position=0, leave=True):\n","    X_input_ids.append(convert_text_to_number_and_fix_length(this_x, tokenizer_mod))\n","    X_input_masks.append(create_mask_for_input(this_x))\n","    X_segment_ids.append([0 for _ in range(max_sequence_length)])\n","\n","  return X_input_ids, X_input_masks, X_segment_ids"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-8T7Dfndvf0Z","colab_type":"text"},"source":["## Clean and tokenize "]},{"cell_type":"code","metadata":{"id":"Ruj1PKWZj005","colab_type":"code","colab":{}},"source":["def clean_text(all_X, tokenizer_mod=None):\n","  nltk_eng_stopwords = nltk.corpus.stopwords.words('english')\n","  converted_X = []\n","  for this_x in all_X:\n","    this_x_lower = this_x.lower()\n","    this_x_no_punc = this_x_lower.translate(str.maketrans('', '', string.punctuation))\n","    if tokenizer_mod == None:\n","      this_x_tokenized = nltk.tokenize.word_tokenize(this_x_no_punc)\n","      this_x_no_stopword = [val for val in this_x_tokenized if not val in nltk_eng_stopwords]\n","      converted_X.append(this_x_no_stopword)\n","    else:\n","      this_x_no_stopword = \" \".join([val for val in nltk.tokenize.word_tokenize(this_x_no_punc) if not val in nltk_eng_stopwords])\n","      this_x_tokenized = tokenizer_mod.tokenize(this_x_no_stopword)\n","      converted_X.append(this_x_tokenized)\n","  return converted_X\n","\n","def convert_y_to_0_and_1(all_Y):\n","  label_number_mapping = {0: [1, 0], 1: [0, 1]}\n","  converted_y = [label_number_mapping[val] for val in all_Y]\n","  return converted_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uqp90zJbtdG7","colab_type":"code","colab":{}},"source":["all_train_X_cleaned_using_bert = clean_text(train_X, bert_tokenizer_instance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BeWULz8hzhY","colab_type":"code","colab":{}},"source":["all_test_X_cleaned_using_bert = clean_text(test_X, bert_tokenizer_instance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"77rTdgAdvnaU","colab_type":"code","colab":{}},"source":["all_train_y_converted = convert_y_to_0_and_1(train_Y)\n","all_test_y_converted = convert_y_to_0_and_1(test_Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HgQX9l3xxzUX","colab_type":"text"},"source":["### Testing BERT results"]},{"cell_type":"code","metadata":{"id":"eMAxUp7Cthrz","colab_type":"code","outputId":"dd958769-b78b-44bb-b6b4-a247c99a85ad","executionInfo":{"status":"ok","timestamp":1588627231151,"user_tz":-270,"elapsed":1568,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["print('X[0] => before: {} and after: {}'.format(train_X[0], all_train_X_cleaned_using_bert[0]))\n","print('Y[0] => before: {} and after: {}'.format(train_Y[0], all_train_y_converted[0]))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["X[0] => before: Unlike many other films, which are disturbing either by dint of their naked unpleasantness (Man Bites Dog) or their sheer violence (most Peckinpah films), Deliverance shocks by its plausibility. Certainly, the buggery scene is pretty straightforward in its unpleasantness, but the film's effect derives far more from its slow build-up and the tangible sense of isolation surrounding the four leads, both before and after everything starts to go wrong. The moment when the canoes pass under the child on the bridge, who does not even acknowledge the men he had earlier played music with, let alone show any sign of human affection towards them, is among the most sinister in modern film. The tension increases steadily throughout the canoe trip, and perseveres even after the final credits - the ending makes the significance of the characters' ordeals horrifically real. The movie's plausibility is greatly aided by the playing of the leads, particularly Ned Beatty and Jon Voight as the victim and reluctant hero respectively. Burt Reynolds, too, has never been better. The film's cultural influence is demonstrable by the number of people who will understand a reference to 'banjo territory' - perhaps only Get Carter has done such an effective hatchet-job on a region's tourist industry. I can think of only a handful of movies which put me into such a serious depression after they had finished - the oppressive atmosphere of Se7en is the best comparison I can think of. Although so much of it is excellent of itself, Deliverance is a classic above all because there are no adequate points of comparison with it - it is unique. and after: ['unlike', 'many', 'films', 'disturbing', 'either', 'din', '##t', 'naked', 'unpleasant', '##ness', 'man', 'bites', 'dog', 'sheer', 'violence', 'peck', '##in', '##pa', '##h', 'films', 'deliver', '##ance', 'shocks', 'pl', '##aus', '##ibility', 'certainly', 'bug', '##ger', '##y', 'scene', 'pretty', 'straightforward', 'unpleasant', '##ness', 'films', 'effect', 'derives', 'far', 'slow', 'build', '##up', 'tangible', 'sense', 'isolation', 'surrounding', 'four', 'leads', 'everything', 'starts', 'go', 'wrong', 'moment', 'canoe', '##s', 'pass', 'child', 'bridge', 'even', 'acknowledge', 'men', 'earlier', 'played', 'music', 'let', 'alone', 'show', 'sign', 'human', 'affection', 'towards', 'among', 'sinister', 'modern', 'film', 'tension', 'increases', 'steadily', 'throughout', 'canoe', 'trip', 'per', '##se', '##vere', '##s', 'even', 'final', 'credits', 'ending', 'makes', 'significance', 'characters', 'ordeal', '##s', 'horrific', '##ally', 'real', 'movies', 'pl', '##aus', '##ibility', 'greatly', 'aided', 'playing', 'leads', 'particularly', 'ned', 'beatty', 'jon', 'vo', '##ight', 'victim', 'reluctant', 'hero', 'respectively', 'burt', 'reynolds', 'never', 'better', 'films', 'cultural', 'influence', 'demons', '##tra', '##ble', 'number', 'people', 'understand', 'reference', 'banjo', 'territory', 'perhaps', 'get', 'carter', 'done', 'effective', 'hatch', '##et', '##jo', '##b', 'regions', 'tourist', 'industry', 'think', 'handful', 'movies', 'put', 'serious', 'depression', 'finished', 'oppressive', 'atmosphere', 'se', '##7', '##en', 'best', 'comparison', 'think', 'although', 'much', 'excellent', 'deliver', '##ance', 'classic', 'adequate', 'points', 'comparison', 'unique']\n","Y[0] => before: 1 and after: [0, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MztG_Ei6x3ul","colab_type":"text"},"source":["# Creating Model"]},{"cell_type":"markdown","metadata":{"id":"CX9WnCmUx58B","colab_type":"text"},"source":["## Using Bert"]},{"cell_type":"code","metadata":{"id":"680fZdfa2qun","colab_type":"code","outputId":"992aa8ee-21b0-418a-9cd2-3c84f13e7a7a","executionInfo":{"status":"ok","timestamp":1588627310519,"user_tz":-270,"elapsed":3541,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_input_ids, train_input_masks, train_segment_ids = create_req_bert_inputs(all_train_X_cleaned_using_bert, bert_tokenizer_instance)\n","test_input_ids, test_input_masks, test_segment_ids = create_req_bert_inputs(all_test_X_cleaned_using_bert, bert_tokenizer_instance)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["100%|██████████| 25000/25000 [00:01<00:00, 15414.33it/s]\n","100%|██████████| 25000/25000 [00:00<00:00, 27253.93it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fRJQNWXDKWyw","colab_type":"code","outputId":"af267698-0c2c-4436-d4da-cd5b20d53fd4","executionInfo":{"status":"ok","timestamp":1588627313576,"user_tz":-270,"elapsed":1926,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["bert_model_preped_input = [np.asarray(train_input_ids, dtype=np.int32), np.asarray(train_input_masks, dtype=np.int32), np.asarray(train_segment_ids, dtype=np.int32)]\n","len(bert_model_preped_input), bert_model_preped_input[0].shape, bert_model_preped_input[0][1].shape"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, (25000, 128), (128,))"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"iREClz0e25nV","colab_type":"code","colab":{}},"source":["this_optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n","    name='Adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpHXcNdNvUvm","colab_type":"code","colab":{}},"source":["def create_model():\n","  input_word_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32,name=\"input_word_ids\")\n","  input_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32,name=\"input_mask\")\n","  segment_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32,name=\"segment_ids\")\n","  bert_layer = hub.KerasLayer(bert_model_hub_link,trainable=True)\n","  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","  pooling_lay = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n","  dense = tf.keras.layers.Dense(768, activation='relu')(pooling_lay)\n","  pred = tf.keras.layers.Dense(2, activation='softmax')(dense)\n","  model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pred)\n","  return model "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQHP6X_h4Pnh","colab_type":"code","colab":{}},"source":["def plot_accuracy_and_loss(file_save_name, history):\n","  plt.plot(history.history['accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.savefig(file_save_name+'_accuracy.png')\n","  plt.cla()\n","  plt.plot(history.history['loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.savefig(file_save_name+'_loss.png')\n","  plt.cla()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGC5twGSdw4D","colab_type":"code","colab":{}},"source":["def evaluate_model(model):\n","  bert_model_test_input = [np.asarray(test_input_ids, dtype=np.int32), np.asarray(test_input_masks, dtype=np.int32), np.asarray(test_segment_ids, dtype=np.int32)]\n","  predicted_y = model.predict(bert_model_test_input)\n","  y_test_conv = [np.argmax(val) for val in all_test_y_converted]\n","  predicted_y_conv = [np.argmax(val) for val in predicted_y]\n","  print('Accuracy:', accuracy_score(y_test_conv, predicted_y_conv))\n","  print('Classification report:', classification_report(y_test_conv, predicted_y_conv))\n","  print('F1 (Micro):', f1_score(y_test_conv, predicted_y_conv, average='micro'))\n","  print('F1 (Macro):', f1_score(y_test_conv, predicted_y_conv, average='macro'))\n","  print('F1 (Weighted):', f1_score(y_test_conv, predicted_y_conv, average='weighted'))\n","  print('Recall (Micro):', recall_score(y_test_conv, predicted_y_conv, average='micro'))\n","  print('Recall (Macro):', recall_score(y_test_conv, predicted_y_conv, average='macro'))\n","  print('Recall (Weighted):', recall_score(y_test_conv, predicted_y_conv, average='weighted'))\n","  print('Precision (Micro):', precision_score(y_test_conv, predicted_y_conv, average='micro'))\n","  print('Precision (Macro):', precision_score(y_test_conv, predicted_y_conv, average='macro'))\n","  print('Precision (Weighted):', precision_score(y_test_conv, predicted_y_conv, average='weighted'))\n","  fpr, tpr, thresholds = metrics.roc_curve(y_test_conv, predicted_y_conv, pos_label=1)\n","  print('AUC:', metrics.auc(fpr, tpr))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41a92QXmo6Kd","colab_type":"text"},"source":["**1 Epoch**\n","\n","\n","```\n","Accuracy: 0.9730941704035875\n","\n","Classification report:               precision    recall  f1-score   support\n","\n","         ham       0.97      1.00      0.98       965\n","        spam       0.99      0.81      0.89       150\n","\n","    accuracy                           0.97      1115\n","   macro avg       0.98      0.90      0.94      1115\n","weighted avg       0.97      0.97      0.97      1115\n","\n","F1 (Micro): 0.9730941704035875\n","F1 (Macro): 0.9371920627290753\n","F1 (Weighted): 0.9719016923313616\n","Recall (Micro): 0.9730941704035875\n","Recall (Macro): 0.9028151986183074\n","Recall (Weighted): 0.9730941704035875\n","Precision (Micro): 0.9730941704035875\n","Precision (Macro): 0.9812994238357023\n","Precision (Weighted): 0.9736217182706349\n","AUC: 0.9028151986183074\n","```\n","\n","**10 Epochs**\n","\n","```\n","Accuracy: 0.8654708520179372\n","Classification report:               precision    recall  f1-score   support\n","\n","         ham       0.87      1.00      0.93       965\n","        spam       0.00      0.00      0.00       150\n","\n","    accuracy                           0.87      1115\n","   macro avg       0.43      0.50      0.46      1115\n","weighted avg       0.75      0.87      0.80      1115\n","\n","F1 (Micro): 0.8654708520179372\n","F1 (Macro): 0.46394230769230765\n","F1 (Weighted): 0.803057088651259\n","Recall (Micro): 0.8654708520179372\n","Recall (Macro): 0.5\n","Recall (Weighted): 0.8654708520179372\n","Precision (Micro): 0.8654708520179372\n","Precision (Macro): 0.4327354260089686\n","Precision (Weighted): 0.7490397956926541\n","AUC: 0.5\n","```\n","\n","**20 Epochs**\n","\n","```\n","Accuracy: 0.8654708520179372\n","Classification report:               precision    recall  f1-score   support\n","\n","         ham       0.87      1.00      0.93       965\n","        spam       0.00      0.00      0.00       150\n","\n","    accuracy                           0.87      1115\n","   macro avg       0.43      0.50      0.46      1115\n","weighted avg       0.75      0.87      0.80      1115\n","\n","F1 (Micro): 0.8654708520179372\n","F1 (Macro): 0.46394230769230765\n","F1 (Weighted): 0.803057088651259\n","Recall (Micro): 0.8654708520179372\n","Recall (Macro): 0.5\n","Recall (Weighted): 0.8654708520179372\n","Precision (Micro): 0.8654708520179372\n","Precision (Macro): 0.4327354260089686\n","Precision (Weighted): 0.7490397956926541\n","AUC: 0.5\n","```\n","\n","**50 Epochs**\n","\n","```\n","Accuracy: 0.8654708520179372\n","Classification report:               precision    recall  f1-score   support\n","\n","         ham       0.87      1.00      0.93       965\n","        spam       0.00      0.00      0.00       150\n","\n","    accuracy                           0.87      1115\n","   macro avg       0.43      0.50      0.46      1115\n","weighted avg       0.75      0.87      0.80      1115\n","\n","F1 (Micro): 0.8654708520179372\n","F1 (Macro): 0.46394230769230765\n","F1 (Weighted): 0.803057088651259\n","Recall (Micro): 0.8654708520179372\n","Recall (Macro): 0.5\n","Recall (Weighted): 0.8654708520179372\n","Precision (Micro): 0.8654708520179372\n","Precision (Macro): 0.4327354260089686\n","Precision (Weighted): 0.7490397956926541\n","AUC: 0.5\n","```"]},{"cell_type":"code","metadata":{"id":"eqAW2g-ZCbGi","colab_type":"code","outputId":"0efadef1-db40-43e1-80d0-0aa06fa2918f","executionInfo":{"status":"ok","timestamp":1588627369332,"user_tz":-270,"elapsed":998,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["val_counts_in_train_y = {}\n","for val in all_train_y_converted:\n","  if not str(val) in val_counts_in_train_y:\n","    val_counts_in_train_y[str(val)] = 0\n","  val_counts_in_train_y[str(val)] += 1\n","val_counts_in_train_y"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'[0, 1]': 12500, '[1, 0]': 12500}"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"N5tuy8OkkspG","colab_type":"code","colab":{}},"source":["all_train_y_converted = np.asarray(all_train_y_converted)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUksObv2zJZ3","colab_type":"code","outputId":"af6d670e-a26d-4d81-eefb-7a529dec8383","executionInfo":{"status":"ok","timestamp":1588621841166,"user_tz":-270,"elapsed":240793,"user":{"displayName":"Ranking R","photoUrl":"","userId":"08956235012566838232"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epochs in number_of_epochs:\n","  print('Number of epochs:', epochs)\n","  model = create_model()\n","  print(model.summary())\n","  model.compile(loss='binary_crossentropy', optimizer=this_optimizer, metrics=['accuracy'])\n","  history = model.fit(\n","      bert_model_preped_input, \n","      all_train_y_converted,\n","      epochs=epochs,\n","      batch_size=batch_size\n","  )\n","  print('Done training')\n","  image_save_file_name = 'drive/My Drive/CA5_Sabri_810198312/Results/Q2_bert_model_{}_epochs'.format(epochs)\n","  plot_accuracy_and_loss(image_save_file_name, history)\n","  evaluate_model(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of epochs: 1\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer_2 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 768)          0           keras_layer_2[0][1]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 768)          590592      global_average_pooling1d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 2)            1538        dense_2[0][0]                    \n","==================================================================================================\n","Total params: 110,074,371\n","Trainable params: 110,074,370\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","None\n","782/782 [==============================] - 345s 441ms/step - loss: 0.7115 - accuracy: 0.5061\n","Done training\n","Accuracy: 0.5\n","Classification report:               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00     12500\n","           1       0.50      1.00      0.67     12500\n","\n","    accuracy                           0.50     25000\n","   macro avg       0.25      0.50      0.33     25000\n","weighted avg       0.25      0.50      0.33     25000\n","\n","F1 (Micro): 0.5\n","F1 (Macro): 0.3333333333333333\n","F1 (Weighted): 0.33333333333333326\n","Recall (Micro): 0.5\n","Recall (Macro): 0.5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Recall (Weighted): 0.5\n","Precision (Micro): 0.5\n","Precision (Macro): 0.25\n","Precision (Weighted): 0.25\n","AUC: 0.5\n","Number of epochs: 10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_average_pooling1d_2 (Glo (None, 768)          0           keras_layer_3[0][1]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 768)          590592      global_average_pooling1d_2[0][0] \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 2)            1538        dense_4[0][0]                    \n","==================================================================================================\n","Total params: 110,074,371\n","Trainable params: 110,074,370\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/10\n","782/782 [==============================] - 344s 440ms/step - loss: 0.7234 - accuracy: 0.4931\n","Epoch 2/10\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6993 - accuracy: 0.5032\n","Epoch 3/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6967 - accuracy: 0.4972\n","Epoch 4/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6962 - accuracy: 0.5019\n","Epoch 5/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6954 - accuracy: 0.4994\n","Epoch 6/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6951 - accuracy: 0.4934\n","Epoch 7/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6951 - accuracy: 0.4992\n","Epoch 8/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6943 - accuracy: 0.4989\n","Epoch 9/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6945 - accuracy: 0.4991\n","Epoch 10/10\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6946 - accuracy: 0.4967\n","Done training\n","Accuracy: 0.5\n","Classification report:               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00     12500\n","           1       0.50      1.00      0.67     12500\n","\n","    accuracy                           0.50     25000\n","   macro avg       0.25      0.50      0.33     25000\n","weighted avg       0.25      0.50      0.33     25000\n","\n","F1 (Micro): 0.5\n","F1 (Macro): 0.3333333333333333\n","F1 (Weighted): 0.33333333333333326\n","Recall (Micro): 0.5\n","Recall (Macro): 0.5\n","Recall (Weighted): 0.5\n","Precision (Micro): 0.5\n","Precision (Macro): 0.25\n","Precision (Weighted): 0.25\n","AUC: 0.5\n","Number of epochs: 20\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer_4 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_average_pooling1d_3 (Glo (None, 768)          0           keras_layer_4[0][1]              \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 768)          590592      global_average_pooling1d_3[0][0] \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 2)            1538        dense_6[0][0]                    \n","==================================================================================================\n","Total params: 110,074,371\n","Trainable params: 110,074,370\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.7192 - accuracy: 0.5004\n","Epoch 2/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6955 - accuracy: 0.5014\n","Epoch 3/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6947 - accuracy: 0.5029\n","Epoch 4/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6949 - accuracy: 0.5006\n","Epoch 5/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6947 - accuracy: 0.4963\n","Epoch 6/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6944 - accuracy: 0.5024\n","Epoch 7/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6938 - accuracy: 0.5019\n","Epoch 8/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6943 - accuracy: 0.4989\n","Epoch 9/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6938 - accuracy: 0.4991\n","Epoch 10/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6936 - accuracy: 0.5018\n","Epoch 11/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6937 - accuracy: 0.4985\n","Epoch 12/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6934 - accuracy: 0.5021\n","Epoch 13/20\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6933 - accuracy: 0.4983\n","Epoch 14/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4974\n","Epoch 15/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4956\n","Epoch 16/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4958\n","Epoch 17/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4979\n","Epoch 18/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4996\n","Epoch 19/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4999\n","Epoch 20/20\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4997\n","Done training\n","Accuracy: 0.5\n","Classification report:               precision    recall  f1-score   support\n","\n","           0       0.50      1.00      0.67     12500\n","           1       0.00      0.00      0.00     12500\n","\n","    accuracy                           0.50     25000\n","   macro avg       0.25      0.50      0.33     25000\n","weighted avg       0.25      0.50      0.33     25000\n","\n","F1 (Micro): 0.5\n","F1 (Macro): 0.3333333333333333\n","F1 (Weighted): 0.33333333333333326\n","Recall (Micro): 0.5\n","Recall (Macro): 0.5\n","Recall (Weighted): 0.5\n","Precision (Micro): 0.5\n","Precision (Macro): 0.25\n","Precision (Weighted): 0.25\n","AUC: 0.5\n","Number of epochs: 50\n","Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer_5 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_average_pooling1d_4 (Glo (None, 768)          0           keras_layer_5[0][1]              \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 768)          590592      global_average_pooling1d_4[0][0] \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 2)            1538        dense_8[0][0]                    \n","==================================================================================================\n","Total params: 110,074,371\n","Trainable params: 110,074,370\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.7198 - accuracy: 0.5022\n","Epoch 2/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6970 - accuracy: 0.4975\n","Epoch 3/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6963 - accuracy: 0.4936\n","Epoch 4/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6954 - accuracy: 0.5007\n","Epoch 5/50\n","782/782 [==============================] - 344s 440ms/step - loss: 0.6956 - accuracy: 0.4957\n","Epoch 6/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6949 - accuracy: 0.4945\n","Epoch 7/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6940 - accuracy: 0.5052\n","Epoch 8/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6940 - accuracy: 0.5088\n","Epoch 9/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6946 - accuracy: 0.5002\n","Epoch 10/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6938 - accuracy: 0.4995\n","Epoch 11/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6941 - accuracy: 0.4992\n","Epoch 12/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6941 - accuracy: 0.4975\n","Epoch 13/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6934 - accuracy: 0.5050\n","Epoch 14/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6937 - accuracy: 0.5026\n","Epoch 15/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6937 - accuracy: 0.4972\n","Epoch 16/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6935 - accuracy: 0.4954\n","Epoch 17/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6934 - accuracy: 0.5022\n","Epoch 18/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6936 - accuracy: 0.4996\n","Epoch 19/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6934 - accuracy: 0.4998\n","Epoch 20/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6933 - accuracy: 0.5019\n","Epoch 21/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6934 - accuracy: 0.4918\n","Epoch 22/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6936 - accuracy: 0.5040\n","Epoch 23/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6933 - accuracy: 0.5005\n","Epoch 24/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6934 - accuracy: 0.4983\n","Epoch 25/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6933 - accuracy: 0.4994\n","Epoch 26/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4982\n","Epoch 27/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4960\n","Epoch 28/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 29/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4966\n","Epoch 30/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4986\n","Epoch 31/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4978\n","Epoch 32/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4982\n","Epoch 33/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.4987\n","Epoch 34/50\n","782/782 [==============================] - 345s 441ms/step - loss: 0.6932 - accuracy: 0.5006\n","Epoch 35/50\n","355/782 [============>.................] - ETA: 3:08 - loss: 0.6932 - accuracy: 0.5004"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2Osr0LYh-cBf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}